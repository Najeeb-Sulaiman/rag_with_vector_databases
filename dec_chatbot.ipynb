{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End Project: Building a WhatsApp-Based Q&A Chatbot with Vector Database and RAG for Data Engineering Community.\n",
        "\n",
        "# Overview:\n",
        "The goal is to build a Question and Answer chatbot using WhatsApp group chat data. This will include cleaning the data, vectorizing it, and implementing a Retrieval-Augmented Generation (RAG) framework using LangChain and a Large Language Model (LLM).\n",
        "\n",
        "# Steps Outline:\n",
        "1. Data Preprocessing and Cleaning (already done and loaded into a CSV).\n",
        "2. Data Preparation\n",
        "3. Embedding Generation and Storage in a Vector Database.\n",
        "3. Implemention of RAG Workflow with LangChain."
      ],
      "metadata": {
        "id": "P7nNhCq6aNNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin, we must install the prerequisite libraries that we will be using in this notebook."
      ],
      "metadata": {
        "id": "qXGkD40nZWDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "pandas==2.2.3 \\\n",
        "pinecone==5.4.2 \\\n",
        "langchain_pinecone==0.2.0 \\\n",
        "langchain-huggingface==0.1.2 \\\n",
        "langchain-community==0.3.13 \\\n",
        "langchain-openai==0.2.14"
      ],
      "metadata": {
        "collapsed": true,
        "id": "axkkHda_Blxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "For this project, we will be using a pre-cleaned dataset of the data engineering community Whatsapp group conversations. This dataset contains about 20,000 group messages. When working with your own dataset, you may need to perform the cleaning step but the dataset has already been cleaned so we can jump right to the action."
      ],
      "metadata": {
        "id": "D_6PYtwwbabs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E_XO7wK8_EAT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load cleaned WhatsApp data from a CSV file.\n",
        "data = pd.read_csv(\"/content/messages.csv\")\n",
        "# Let's format the dataset to extract and concatinate the sender and message.\n",
        "data['content'] = data['sender'].astype(str) + ': ' + data['message'].astype(str)\n",
        "data = data['content'].tolist()\n",
        "# Using a subset of the full dataset.\n",
        "data = data[4:6]\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate embeddings\n",
        " Now, let's generate sample embedding for a list of texts using a subset of the full dataset. We achieve this by using an open source embedding model `all-MiniLM-L6-v2` from HuggingFace. It is free to use this model unlike the OpenAI embedding models which cost some penny."
      ],
      "metadata": {
        "id": "huTz5nlvgZ0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "# Instantiate a model object\n",
        "model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
        "# Use the model object to generate embeddings\n",
        "embeddings = model.embed_query(data[0])\n",
        "print(\"Generated Embeddings:\", pd.DataFrame(embeddings))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yoPmHpO7XeDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the Index in a Vector Database\n",
        "Index are vector stores for embeddings and they enable efficient search on these vectors. To persist these vector stores we use a vector database. We will be using Pinecone as our vector database. It is one of the most popular vector databases and offers generous free tier. To interact with pinecone databases we need an API key, we can get a [free API key](https://:app.pinecone.io) to initialize a connection to Pinecone and create an index."
      ],
      "metadata": {
        "id": "hkN5oy-4kKtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pinecone import ServerlessSpec, Pinecone\n",
        "\n",
        "# get API key stored in environment variable in colab secrets\n",
        "api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "# initialize connection to pinecone\n",
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "XOFYYLPnBXW7"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to setup our index specification by defining the cloud provider and region where we want to deploy our serverless index."
      ],
      "metadata": {
        "id": "4vP7m6e3sKxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "index_name = 'dec-chat-index'"
      ],
      "metadata": {
        "id": "vY0kl74fB7XF"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for and delete index if already exists\n",
        "if index_name in pc.list_indexes().names():\n",
        "    pc.delete_index(index_name)"
      ],
      "metadata": {
        "id": "dUuIKie1DXir"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the index exists, if not, create it\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=384, # Dimension for SentenceTransformer embeddings\n",
        "        metric='cosine', # Vectors similarity search metric\n",
        "        spec=spec)"
      ],
      "metadata": {
        "id": "ruEzr1DaCetA"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "erky2o3rCtMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should see that the new Pinecone index has been created and has a `total_vector_count` of 0, as we haven't added any vectors yet."
      ],
      "metadata": {
        "id": "CidHg5Qtu1vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add data to the vector store\n",
        "Let's add data to the vector store by using the langchain's `PineconeVectorStore` class. Once we have initialized a PineconeVectorStore object, we can add records to the underlying Pinecone index (and thus also the linked LangChain object) using either the `add_documents` or `add_texts` methods.\n",
        "Both of these methods also handle the embedding of the provided text data and the creation of records in the Pinecone index."
      ],
      "metadata": {
        "id": "BEjXr8_lvkl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample iteration to demonstrate how we will iterate over our data in other to generate texts embeddings and store them.\n",
        "for i, embedding in enumerate(data):\n",
        "  print(i, embedding)"
      ],
      "metadata": {
        "id": "nXquaI7zF7jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "# Add data to the vector store\n",
        "vector_store = PineconeVectorStore(index, embedding=model, text_key=\"text\")\n",
        "for i, embedding in enumerate(data):\n",
        "    vector_store.add_texts([data[i]], metadatas=[{\"id\": i}])"
      ],
      "metadata": {
        "id": "1MqG4Yz_C78s"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, Let's check the number of vectors in our index."
      ],
      "metadata": {
        "id": "sfZlJIr9yOgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index.describe_index_stats()"
      ],
      "metadata": {
        "id": "jLigstNBbysY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's play with some similarity search on our vector index."
      ],
      "metadata": {
        "id": "ce2QnHbryf9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did Najeeb say\"\n",
        "query = \"Which community was everyone welcomed to?\"\n",
        "query = \"How many percent response do we have for the survey?\"\n",
        "query = \"What is data engineering?\"\n",
        "\n",
        "vector_store.similarity_search(\n",
        "    query,  # our search query\n",
        "    k=3  # return 3 most relevant docs\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "A7MujfXjb12a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Workflow Implementation\n",
        "To create our Q&A chatbot we need to create a Retrieval-Augmented Generation chain using LangChain. We will integrate with different LLMs for demonstration of how each of these LLMs performs with text generation."
      ],
      "metadata": {
        "id": "8KipxRqvzAbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with the `tiiuae/falcon-7b-instruct` LLM from HuggingFace. Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets."
      ],
      "metadata": {
        "id": "jxkXAtqAPdsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Create a Retrieval-Augmented Generation chain using LangChain.\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
        "    temperature= 0.01,\n",
        "    max_new_tokens=250,\n",
        "    huggingfacehub_api_token=userdata.get('HF_TOKEN'))\n",
        "retriever = vector_store.as_retriever()\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                       chain_type=\"stuff\",\n",
        "                       retriever=retriever)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ia9fO_s2ddC0"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a query against the RAG pipeline\n",
        "query = \"How many percent response do we have for the survey?\"\n",
        "qa_chain.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RgjYRgiKQ_gh",
        "outputId": "b355d0a0-8158-4b28-e4d6-2d21657865f8"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'How many percent response do we have for the survey?',\n",
              " 'result': 'less than 20%'}"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test performance with `google/flan-t5-large` LLM from HuggingFace. It is one of the leading LLMs on HuggingFace with about 1B model size."
      ],
      "metadata": {
        "id": "KUgcjATYSwAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "# Create a Retrieval-Augmented Generation chain using LangChain.\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-large\",\n",
        "    model_kwargs={\"temperature\": 0.01, \"max_new_tokens\": 250},\n",
        "    huggingfacehub_api_token=userdata.get('HF_TOKEN'))\n",
        "retriever = vector_store.as_retriever()\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                       chain_type=\"stuff\",\n",
        "                       retriever=retriever)"
      ],
      "metadata": {
        "id": "qr1hwXxSwLo1"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a query against the RAG pipeline\n",
        "query = \"How many percent response do we have for the survey?\"\n",
        "qa_chain.invoke(query)"
      ],
      "metadata": {
        "id": "1PxIOOkbUniW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we will test performance with the popular OpenAI `gpt-4o-mini` model. This model has more than 1 trillion parameters"
      ],
      "metadata": {
        "id": "bSNazoFGUxMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# chatbot language model\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=userdata.get('OPENAI_API_KEY'),\n",
        "    model_name='gpt-4o-mini',\n",
        "    temperature=0.0\n",
        ")\n",
        "# retrieval augmented pipeline for chatbot\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "8oGYhWDrmaaV"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a query against the RAG pipeline\n",
        "query = \"How many percent response do we have for the survey?\"\n",
        "qa.invoke(query)\n",
        "#qa.run(query)"
      ],
      "metadata": {
        "id": "VZwlVnjzkqKV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}